{% extends "base.html" %}

{% block content %}
    <!-- Page Header -->
    <header class="masthead" style="background-image: url('static/img/tunnel2.jpg')">
      <div class="overlay"></div>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <div class="post-heading">
              <h1>Down the rabbit hole I go.</h1>
              <h2 class="subheading">I've always been a firm believer in understanding the core technology behind any solution.</h2>
              <span class="meta">Posted by
                <a href="#">Sean Gill</a>
                on September 10, 2018</span>
            </div>
          </div>
        </div>
      </div>
    </header>

    <!-- Post Content -->
    <article>
      <div class="container">
        <div class="row">
          <div class="col-lg-8 col-md-10 mx-auto">
            <p>As a Database Administrator you learn that whenever there is a problem with an application the first two groups to get blamed are the DBAs and Networking. I mean, heaven forbid a Developer makes a mistake and releases some epic piece of shit. Yeah, yeah, yeah, it worked fine in Dev and performs like a champ on your laptop, I get it.
              I found the best way to convince developers that the database was not the issue was to have solid metrics of how their code performed before and after a Production release. I am a data guy and the data generally doesn't lie. Sure it can be manipulated and misrepresented but that is another post for another day. Gathering those metrics
              isn't as easy as it sounds. Yes there are a great number of software products out there that capture just about every database metric you could want, but they also capture a good deal of your Company's money which is something I haven't always had the luxury of getting for monitoring products. So, I had to "think like the database". But since
              the database sits on top of the Operating System, I first had to learn how to "think like the Operating System". Finally, the database, the application and the end user are not all connected to the same server so I needed to learn enough about Networking to be able to glue the pieces together. A tremendous amount of effort and learning
              but the knowledge I have been able to garner over the years had proven itself invaluable when said developers release said epic pieces of shit and try to blame everyone else under the sun.</p>

            <p>So what was the whole point behind that mini-rant? In a nutshell, it made me understand the importance of having a firm grasp of the overall architecture when either troubleshooting or building from scratch. Which leads me to this web-site and the opening of the rabbit hole.</p>

            <p>As the Enterprise Data Architect for my employer, my charge has been to lead the company into the next generation of Advanced Analytics. I believe most people would agree the next generation of just about anything I.T. related lives in the Cloud, even if it is in Hybrid form. The problem was that as of two years ago, I had absolutely no idea
              what I was talking about when it came to Cloud-based solutions. Don't get me wrong, I understood what they were and why people were going there but I couldn't hold a decent conversation beyond what I had read about online. Fast-forward about 4 months from that initial "what the hell am I doing" jumping point and we decided on AWS as our platform.
              There were a number of reasons behind us choosing AWS over the others but in the end it boiled down to which platform had the most offerings that were open-source. <b><i>SPOILER ALERT....open-source does not mean FREE!</b></i> We had spoken to a number of people from both industry and academia and decided that we would use S3 as storage for everything
             (i.e. the DataLake) and then use Spark, Athena and possibly Presto for accesing and processing the data and Python as our language of choice. Sounds great. How do I build all this stuff? Well, once we started talking about things like VPCs, Internet Gateways, Subnet Groups, NAT Instances, VPN Gateways, Database Migration Services, EMR....I think you get it.
              There was a ton to learn. Now did I have to learn all those things in order to process data with PySpark? Not at all, but remember what I said about learning the overall architecture before trying to build something from scratch. I just took my first steps into the rabbit hole.</p>

              <p>Learning AWS can be a bit frustrating at times. Just about every vendor trying to sell you an AWS based solution will happily tell you that you could do everything their app does if you are willing to spend the time and effort designing and coding all the "AWS Lego Blocks" required. Pretty sure they all went to the same conference that mentioned "AWS Lego Blocks"
              because they all mention "AWS Lego Blocks". But they are not wrong. Have you ever bought a toy for a child that has "some assembly required" and the instructions suck? Welcome to AWS documentation. My first real foray into the AWS realm was with Database Migration Services (DMS) and AWS Glue. DMS against Microsoft SQL Server is essentially SQL Replication without all
              the bells and whistles. Glue, as I understood, was Python-based but actually uses PySpark (they have since added Scala) that runs against a "serverless" EMR Cluster. Oh, and manipulating files in S3 is about as close to manipulating files locally as jumping from the second to last stair to the ground and sky-diving. Ok, maybe not that bad but between Glue and S3 file
             manipulation, I took a serious stumble down the rabbit hole which was getting increasingly darker. A few months later and I have all the Glue ETL prepping data on a nightly basis. The architecture can support more frequent loading but we are still trying to keep the costs at a minimum. Now it is on to Spark processing. Quick poll....how many people think that you can take
            Glue code, which is running on a Spark Cluster and run it on a true Spark Cluster? If you answered, "Fuck no that would be too easy" then you are correct. You see, AWS Glue uses a custom Python Module that ONLY RUNS INSIDE GLUE. You cannot import it like any other God-damn Python Module. I hate this freaking rabbit hole. Second poll....If you create a Spark Cluster configured
             to use the Glue catalog for metadata and attach a Zeppelin Notebook, do you think the code you write in the Zeppelin notebook will run as-is in a Spark job via "spark-aubmit"? If you answered, "Fuck no that would be too easy" then you are correct again! Congratulations. This rabbit hole officially sucks.</p>

             <p>Fast-forward another few months and the rabbit hole is beginning to feel more like home. I know the tweaks and tricks required to have my code run between the different flavors of PySpark and have introduced Lambda, CloudWatch and SNS for automation and alerting. My knowledge of Spark has progressed to the point where I am using custom bootstraps and .json config files to
               increase performance and load custom Python and Scala modules, am utilizing Spot Instances to keep costs low and have auto-scaling policies configured to scale up during increased processing and to also scale back down as the load decreases. We have started playing with Kinesis to develop a semi-ESB and look forward to getting some Spark Streaming
               under our belt. I have custom IAM policies in place to restrict access and my biggest access concern is that my Company's Firewall rules prevent us from SSHing outside of a particular VPC from inside our internal network. Given the high visibility of everything hacked inside the Cloud, I can't blame them. Now on to the next challenge. I have the data prepped and ready to
               rock but how are people going to access the fruits of my labor outside of reaching in from Tableau and Business Objects. Will we get killed in egress charges? We are a very immature company from a Data Analyst perspective so having folks bounce around between Athena, Zeppelin and Hue would be a recipe for disaster. For some unknown reason, upper management is hell-bent on
               not buying any consolidated front end tool like Qubole so what are the options? I know, let me see if I can design something from scratch using some more "AWS Lego Blocks" and Python. Ladies and Gentlemen, welcome to the latest segment of my rabbit hole, this website. In all honesty, it hasn't been too bad considering I suck at developing Presentation Layer tools. In fact,
               I pretty much refuse to do reporting bullshit at work and made that a requirement before moving over to my latest group. But I digress. This has been a labor of love since I have developed this 100% on my own time, using my own AWS account. In the two weeks since I had this brilliant idea, I have fully utilized GitHub (we use TFS at work), Flask, Bootstrap4, Jinja2,
               Elastic Load Balancing and Auto-Scaling Groups (you know...in case this thing takes off, I don't want to be caught off-guard j/k), Elastic Beanstalk and more JavaScript and JQuery than I ever anticipated.</p>

               <p>Guess everything wasn't as simple as "We're going to use Spark in AWS to process our data for Advanced Analytics". No regrets though.</p>

          </div>
        </div>
      </div>
    </article>

    <hr>
{% endblock %}
